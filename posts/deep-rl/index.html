<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="peronal-blog"><meta property="og:title" content="Playing Atari with deep reinforcement learning "><meta property="og:description" content="peronal-blog"><meta property="og:type" content="article"><meta property="og:url" content="https://fivetwentythree.github.io/personal-blog/posts/deep-rl/"><meta name=description content="peronal-blog"><link rel="shortcut icon" href=https://fivetwentythree.github.io/personal-blog/favicon.ico><link rel=stylesheet href=https://fivetwentythree.github.io/personal-blog/css/style.css><link rel=canonical href=https://fivetwentythree.github.io/personal-blog/posts/deep-rl/><title>Playing Atari with deep reinforcement learning</title><link rel=preload href=https://fivetwentythree.github.io/personal-blog/fonts/departure-mono.woff2 as=font type=font/woff2 crossorigin><script type=text/x-mathjax-config>
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$','$'], ['\\(','\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            // Center equations by default, but allow alignment with tags.
            "HTML-CSS": {
                styles: {
                    ".MathJax_Display": {
                        "text-align": "center"
                    }
                }
            }
        });
    </script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script><meta property="og:title" content="Playing Atari with deep reinforcement learning "><meta property="og:description" content="peronal-blog"><meta property="og:type" content="article"><meta property="og:url" content="https://fivetwentythree.github.io/personal-blog/posts/deep-rl/"><meta property="og:image" content="https://fivetwentythree.github.io/personal-blog/images/atari-breakout-display.jpg_hu_c8b477a36d25fa4b.webp"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Playing Atari with deep reinforcement learning "><meta name=twitter:description content="peronal-blog"><meta name=twitter:image content="https://fivetwentythree.github.io/personal-blog/images/atari-breakout-display.jpg_hu_c8b477a36d25fa4b.webp"></head><body><header id=banner><h2><a href=https://fivetwentythree.github.io/personal-blog/>Lochana Perera</a></h2><nav><ul><li><a href=https://fivetwentythree.github.io/personal-blog/ title=posts>posts</a></li><li><a href=https://fivetwentythree.github.io/personal-blog/about/ title=about>about</a></li></ul></nav></header><main id=content><article><header id=post-header><h1>Playing Atari with deep reinforcement learning</h1><div><time>July 16, 2025</time></div></header><span class="sidenote sidenote-right"><p><img src=https://fivetwentythree.github.io/personal-blog/assets/images/atari-breakout-display.jpg.webp alt=atari-breakout srcset="https://fivetwentythree.github.io/personal-blog/images/atari-breakout-display.jpg_hu_f9658db51bb3bea2.webp 200w, https://fivetwentythree.github.io/personal-blog/images/atari-breakout-display.jpg_hu_c2fe099d017affa1.webp 400w, https://fivetwentythree.github.io/personal-blog/images/atari-breakout-display.jpg_hu_be4ed6fd74fad76.webp 800w, https://fivetwentythree.github.io/personal-blog/images/atari-breakout-display.jpg_hu_a2c0896f080346b2.webp 1200w" sizes="(max-width: 600px) 100vw, 600px" loading=lazy></p><h3 id=atari-breakout>Atari Breakout</h3><p>Breakout is a seminal arcade video game developed by Atari, Inc., and released in 1976. Designed by Nolan Bushnell and Steve Bristow, with engineering by Steve Wozniak, it evolved from Atari&rsquo;s Pong and became a cornerstone of early gaming.</p><p><strong>Gameplay</strong>: Players control a horizontal paddle at the bottom of the screen to bounce a ball toward a wall of colored bricks at the top. The ball destroys bricks on contact, scoring points, while deflecting off walls and the paddle. The goal is to clear all bricks across levels without letting the ball fall past the paddle (losing a life). Power-ups and speed increases add challenge; it&rsquo;s single-player with simple controls (knob or joystick).</p><p><strong>Mechanics and Features</strong>:</p><ul><li><strong>Core Loop</strong>: Ball physics simulate realistic bounces; breaking bricks can accelerate the ball or introduce multiples.</li><li><strong>Scoring</strong>: Points vary by brick color/position; high scores encourage replay.</li><li><strong>Lives and Progression</strong>: Start with limited lives; levels advance with faster gameplay and varied brick layouts.</li><li><strong>Hardware</strong>: Originally an arcade cabinet with black-and-white display (color overlays added); ports to Atari 2600 (1978) popularized home versions.</li></ul></span><p>The paper introduces a novel agent, which they call a Deep Q-Network (DQN), that learns to play Atari games by looking only at the screen pixels, just like a human. The key innovations were:
<span class="sidenote sidenote-left"><img src=https://fivetwentythree.github.io/personal-blog/assets/images/atari-paper.png alt="atari paper" srcset="https://fivetwentythree.github.io/personal-blog/images/atari-paper_hu_7a806f9ce82d4c1f.webp 200w, https://fivetwentythree.github.io/personal-blog/images/atari-paper_hu_b66928f20385b477.webp 400w, https://fivetwentythree.github.io/personal-blog/images/atari-paper_hu_80563ffb8ab6f921.webp 800w, https://fivetwentythree.github.io/personal-blog/images/atari-paper_hu_1aebfc9ec2717ebf.webp 1200w" sizes="(max-width: 600px) 100vw, 600px" loading=lazy></span></p><p>1.End-to-End Learning: Using a deep Convolutional Neural Network (CNN) to directly process raw pixels and output action values, eliminating the need for hand-crafted features.</p><p>2.Experience Replay: Storing the agent&rsquo;s experiences (state, action, reward, next state) in a memory buffer and then randomly sampling from this buffer to train the network. This breaks the correlation between consecutive samples and smooths the data distribution, stabilizing learning.</p><p>3.Target Network (A crucial detail from the follow-up 2015 paper, but implied here): Using a separate, fixed network to generate the target Q-values for the Bellman update. This prevents the target from rapidly changing, which further stabilizes training.</p><h3 id=lets-build-this-components-in-python>lets build this components in python</h3><p>Prerequisites</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>pip</span> <span class=n>install</span> <span class=n>torch</span> <span class=n>gymnasium</span><span class=p>[</span><span class=n>atari</span><span class=p>]</span> <span class=n>ale</span><span class=o>-</span><span class=n>py</span> <span class=n>numpy</span> <span class=n>opencv</span><span class=o>-</span><span class=n>python</span>
</span></span></code></pre></div><h3 id=step-1-setting-up-the-environment--preprocessing>Step 1: Setting up the Environment & Preprocessing</h3><p>The paper specifies a series of preprocessing steps to make learning more manageable.</p><ul><li>Convert the image to grayscale.</li><li>Downsample the image to 110x84.</li><li>Crop the image to an 84x84 region that captures the play area.</li><li>Stack the last 4 frames together to give the network a sense of motion (e.g., the ball&rsquo;s velocity).</li></ul><p>â €We can encapsulate this logic in a gymnasium wrapper.
<span class="sidenote sidenote-right">How to use it:
env = gym.make(&ldquo;ALE/Breakout-v5&rdquo;)
Use the full name with &ldquo;ALE/&rdquo;
processed_env = AtariPreprocessing(env)
state, info = processed_env.reset()
print(f"Observation shape: {state.shape}") # Should be (4, 84, 84)</span></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>gymnasium</span> <span class=k>as</span> <span class=nn>gym</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>cv2</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>collections</span> <span class=kn>import</span> <span class=n>deque</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>AtariPreprocessing</span><span class=p>(</span><span class=n>gym</span><span class=o>.</span><span class=n>Wrapper</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    A wrapper for Atari environments that preprocesses observations as described
</span></span></span><span class=line><span class=cl><span class=s2>    in the DQN paper.
</span></span></span><span class=line><span class=cl><span class=s2>    - Grayscale and resize frames
</span></span></span><span class=line><span class=cl><span class=s2>    - Stack the last 4 frames
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>env</span><span class=p>,</span> <span class=n>frame_stack_size</span><span class=o>=</span><span class=mi>4</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>env</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>frame_stack_size</span> <span class=o>=</span> <span class=n>frame_stack_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>frames</span> <span class=o>=</span> <span class=n>deque</span><span class=p>([],</span> <span class=n>maxlen</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>frame_stack_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Define the new observation space shape</span>
</span></span><span class=line><span class=cl>        <span class=c1># The shape is (frame_stack_size, height, width)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>observation_space</span> <span class=o>=</span> <span class=n>gym</span><span class=o>.</span><span class=n>spaces</span><span class=o>.</span><span class=n>Box</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>low</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>high</span><span class=o>=</span><span class=mi>255</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>            <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>frame_stack_size</span><span class=p>,</span> <span class=mi>84</span><span class=p>,</span> <span class=mi>84</span><span class=p>),</span> 
</span></span><span class=line><span class=cl>            <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>uint8</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_preprocess_frame</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>frame</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Convert to grayscale</span>
</span></span><span class=line><span class=cl>        <span class=n>frame</span> <span class=o>=</span> <span class=n>cv2</span><span class=o>.</span><span class=n>cvtColor</span><span class=p>(</span><span class=n>frame</span><span class=p>,</span> <span class=n>cv2</span><span class=o>.</span><span class=n>COLOR_RGB2GRAY</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Resize to 110x84, then crop to 84x84</span>
</span></span><span class=line><span class=cl>        <span class=n>frame</span> <span class=o>=</span> <span class=n>cv2</span><span class=o>.</span><span class=n>resize</span><span class=p>(</span><span class=n>frame</span><span class=p>,</span> <span class=p>(</span><span class=mi>84</span><span class=p>,</span> <span class=mi>84</span><span class=p>),</span> <span class=n>interpolation</span><span class=o>=</span><span class=n>cv2</span><span class=o>.</span><span class=n>INTER_AREA</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>frame</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>reset</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Reset the underlying environment</span>
</span></span><span class=line><span class=cl>        <span class=n>observation</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>env</span><span class=o>.</span><span class=n>reset</span><span class=p>(</span><span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Preprocess the first frame and fill the deque</span>
</span></span><span class=line><span class=cl>        <span class=n>processed_frame</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_preprocess_frame</span><span class=p>(</span><span class=n>observation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>frame_stack_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>frames</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>processed_frame</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_get_observation</span><span class=p>(),</span> <span class=n>info</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>action</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>observation</span><span class=p>,</span> <span class=n>reward</span><span class=p>,</span> <span class=n>terminated</span><span class=p>,</span> <span class=n>truncated</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>env</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Preprocess the new frame and add it to the deque</span>
</span></span><span class=line><span class=cl>        <span class=n>processed_frame</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_preprocess_frame</span><span class=p>(</span><span class=n>observation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>frames</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>processed_frame</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_get_observation</span><span class=p>(),</span> <span class=n>reward</span><span class=p>,</span> <span class=n>terminated</span><span class=p>,</span> <span class=n>truncated</span><span class=p>,</span> <span class=n>info</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_get_observation</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Return the stacked frames as a single numpy array</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>frames</span><span class=p>)</span>
</span></span></code></pre></div></article></main><footer id=footer></footer></body></html>